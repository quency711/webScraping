{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import urllib2\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "import urllib.request\n",
    "\n",
    "def StringListSave(save_path, filename, slist):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    path = save_path+\"/\"+filename+\".txt\"\n",
    "    with open(path, \"w+\") as fp:\n",
    "        for s in slist:\n",
    "            fp.write(\"%s\\t\\t%s\\n\" % (s[0].encode(\"utf8\"), s[1].encode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Page_Info(myPage):\n",
    "    '''Regex'''\n",
    "    ##  .*?代表什么都可以，加括号变成(.*?) 表示要return的值\n",
    "    mypage_Info = re.findall(r'<div class=\"titleBar\" id=\".*?\"><h2>(.*?)</h2><div class=\"more\"><a href=\"(.*?)\">.*?</a></div></div>', myPage, flags=0)\n",
    "    return mypage_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_Page_Info(new_page):\n",
    "    '''Regex(slowly) or Xpath(fast)'''\n",
    "    # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\\.html\".*?>(.*?)</a></td>', new_page, re.S)\n",
    "    # # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\">(.*?)</a></td>', new_page, re.S) # bugs\n",
    "    # results = []\n",
    "    # for url, item in new_page_Info:\n",
    "    #     results.append((item, url+\".html\"))\n",
    "    # return results\n",
    "    dom = etree.HTML(new_page)\n",
    "    new_items = dom.xpath('//tr/td/a/text()')\n",
    "    new_urls = dom.xpath('//tr/td/a/@href')\n",
    "    assert(len(new_items) == len(new_urls))\n",
    "    return zip(new_items, new_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spider(url):\n",
    "    i = 0\n",
    "    print(\"downloading \", url)\n",
    "    #获取网页内容，用decode gbk获取中文信息\n",
    "    myPage = requests.get(url).content.decode(\"gbk\")\n",
    "    # myPage = urllib2.urlopen(url).read().decode(\"gbk\")\n",
    "    myPageResults = Page_Info(myPage)\n",
    "    save_path = u\"网易新闻抓取\"\n",
    "    filename = str(i)+\"_\"+u\"新闻排行榜\"\n",
    "    StringListSave(save_path, filename, myPageResults)\n",
    "    i += 1\n",
    "    for item, url in myPageResults:\n",
    "        print(\"downloading \", url)\n",
    "        new_page = requests.get(url).content.decode(\"gbk\")\n",
    "        # new_page = urllib2.urlopen(url).read().decode(\"gbk\")\n",
    "        newPageResults = New_Page_Info(new_page)\n",
    "        filename = str(i)+\"_\"+item\n",
    "        StringListSave(save_path, filename, newPageResults)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x259b59fdf08>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def New_Page_Info(new_page):\n",
    "    '''Regex(slowly) or Xpath(fast)'''\n",
    "    # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\\.html\".*?>(.*?)</a></td>', new_page, re.S)\n",
    "    # # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\">(.*?)</a></td>', new_page, re.S) # bugs\n",
    "    # results = []\n",
    "    # for url, item in new_page_Info:\n",
    "    #     results.append((item, url+\".html\"))\n",
    "    # return results\n",
    "    dom = etree.HTML(new_page)\n",
    "    new_items = dom.xpath('//tr/td/a/text()')\n",
    "    new_urls = dom.xpath('//tr/td/a/@href')\n",
    "    assert(len(new_items) == len(new_urls))\n",
    "    return zip(new_items, new_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "downloading  http://news.163.com/rank/\n",
      "downloading  http://news.163.com/special/0001386F/rank_whole.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_news.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_ent.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_sports.html\n",
      "downloading  http://money.163.com/special/002526BH/rank.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_tech.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_auto.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_lady.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_house.html\n",
      "downloading  http://news.163.com/special/0001386F/game_rank.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_travel.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_edu.html\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "start_url = \"http://news.163.com/rank/\"\n",
    "Spider(start_url)\n",
    "print( \"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
